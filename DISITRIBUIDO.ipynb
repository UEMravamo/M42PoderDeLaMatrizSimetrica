{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m col, \u001b[38;5;28msum\u001b[39m \u001b[38;5;28;01mas\u001b[39;00m spark_sum\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StructType, StructField, IntegerType, DoubleType\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\n",
    "\n",
    "# ============== CÓDIGO BASE ==============\n",
    "\n",
    "def mult_matrices_local(A, B, n):\n",
    "    C = [[0]*n for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            s = 0\n",
    "            for k in range(n):\n",
    "                s += A[i][k] * B[k][j]\n",
    "            C[i][j] = s\n",
    "    return C\n",
    "\n",
    "def simplificar_exponente_local(A, k, n):\n",
    "    if k == 1:\n",
    "        return A\n",
    "    elif k % 2 == 0:\n",
    "        mitad = simplificar_exponente_local(A, k // 2, n)\n",
    "        return mult_matrices_local(mitad, mitad, n)\n",
    "    else:\n",
    "        return mult_matrices_local(A, simplificar_exponente_local(A, k - 1, n), n)\n",
    "\n",
    "def generar_matriz_simetrica(n):\n",
    "    M = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            v = np.random.randn() * (-1)**(i + j)\n",
    "            M[i, j] = v\n",
    "            M[j, i] = v\n",
    "    for i in range(n):\n",
    "        M[i, i] += 1\n",
    "    return np.round(M, 2)\n",
    "\n",
    "def matriz_a_df(spark, M):\n",
    "    n = len(M)\n",
    "    data = []\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            data.append((i, j, float(M[i][j])))\n",
    "    schema = StructType([\n",
    "        StructField(\"row\", IntegerType(), False),\n",
    "        StructField(\"col\", IntegerType(), False),\n",
    "        StructField(\"value\", DoubleType(), False)\n",
    "    ])\n",
    "    return spark.createDataFrame(data, schema)\n",
    "\n",
    "def mult_matrices_df(dfA, dfB):\n",
    "    join_df = dfA.alias(\"A\").join(dfB.alias(\"B\"), col(\"A.col\") == col(\"B.row\"))\n",
    "    df_mult = join_df.withColumn(\"partial\", col(\"A.value\") * col(\"B.value\"))\n",
    "    df_res = df_mult.groupBy(col(\"A.row\").alias(\"i\"), col(\"B.col\").alias(\"j\")) \\\n",
    "                    .agg(spark_sum(\"partial\").alias(\"value\"))\n",
    "    df_res = df_res.filter(col(\"i\") <= col(\"j\"))\n",
    "    df_res = df_res.select(col(\"i\").alias(\"row\"), col(\"j\").alias(\"col\"), col(\"value\"))\n",
    "    return df_res\n",
    "\n",
    "def exponenciacion_distribuida(dfA, k, n, spark):\n",
    "    I = np.eye(n)\n",
    "    dfI = matriz_a_df(spark, I)\n",
    "    df_res = dfI\n",
    "    df_pow = dfA\n",
    "    bin_k = bin(k)[2:]\n",
    "    for bit in reversed(bin_k):\n",
    "        if bit == '1':\n",
    "            df_res = mult_matrices_df(df_res, df_pow)\n",
    "        df_pow = mult_matrices_df(df_pow, df_pow)\n",
    "    return df_res\n",
    "\n",
    "# ============== TESTS Y COMPARACIÓN ==============\n",
    "\n",
    "def test_local(A, k):\n",
    "    n = len(A)\n",
    "    t0 = time.time()\n",
    "    R = simplificar_exponente_local(A, k, n)\n",
    "    t1 = time.time()\n",
    "    return R, (t1 - t0)\n",
    "\n",
    "def test_distribuido(M, k, spark):\n",
    "    n = len(M)\n",
    "    dfA = matriz_a_df(spark, M)\n",
    "    t0 = time.time()\n",
    "    df_res = exponenciacion_distribuida(dfA, k, n, spark)\n",
    "    t1 = time.time()\n",
    "    # Reconstruir matriz local\n",
    "    R = [[0]*n for _ in range(n)]\n",
    "    for row in df_res.collect():\n",
    "        i, j, v = row[\"row\"], row[\"col\"], row[\"value\"]\n",
    "        R[i][j] = v\n",
    "        R[j][i] = v\n",
    "    return R, (t1 - t0)\n",
    "\n",
    "def comparar(n, k, spark):\n",
    "    A = generar_matriz_simetrica(n)\n",
    "    A_list = A.tolist()\n",
    "    \n",
    "    r_local, t_local = test_local(A_list, k)\n",
    "    r_dist, t_dist = test_distribuido(A, k, spark)\n",
    "    \n",
    "    print(f\"Matriz {n}x{n}, potencia={k}\")\n",
    "    print(f\"  Tiempo Local: {t_local:.6f} s\")\n",
    "    print(f\"  Tiempo Distribuido: {t_dist:.6f} s\")\n",
    "    \n",
    "    # (Opcional) Comparar resultados\n",
    "    diff = 0\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            diff += abs(r_local[i][j] - r_dist[i][j])\n",
    "    print(f\"  Diferencia acumulada: {diff:.6f}\")\n",
    "    print(\"-\"*40)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"ExponenciarMatricesDist\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "  \n",
    "    configuraciones = [\n",
    "        (1, 1), (1, 5), (1, 10),\n",
    "        (5, 1), (5, 5), (5, 10),\n",
    "        (10, 1), (10, 5), (10, 10),\n",
    "        (20, 1), (20, 5), (20, 10),\n",
    "        (50, 1), (50, 5), (50, 10),\n",
    "        (100, 1), (100, 5), (100, 10),\n",
    "        (200,6), (300,8),(400,9)\n",
    "    ]\n",
    "\n",
    "    for (n, k) in configuraciones:\n",
    "        comparar(n, k, spark)\n",
    "\n",
    "    spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
